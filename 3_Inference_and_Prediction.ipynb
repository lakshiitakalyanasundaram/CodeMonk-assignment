{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Prediction\n",
    "\n",
    "This notebook is used to load the trained model and test it on new images, such as screenshots from Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import joblib\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, num_genders, num_colors, num_seasons, num_products):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gender = nn.Linear(512, num_genders)\n",
    "        self.color = nn.Linear(512, num_colors)\n",
    "        self.season = nn.Linear(512, num_seasons)\n",
    "        self.product = nn.Linear(512, num_products)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).squeeze()\n",
    "        x = self.dropout(x)\n",
    "        return {\n",
    "            'gender': self.gender(x),\n",
    "            'color': self.color(x),\n",
    "            'season': self.season(x),\n",
    "            'product': self.product(x)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# üîÅ Load everything\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gender_enc = joblib.load(\"gender_encoder.pkl\")\n",
    "color_enc = joblib.load(\"baseColour_encoder.pkl\")\n",
    "season_enc = joblib.load(\"usage_encoder.pkl\")\n",
    "product_enc = joblib.load(\"masterCategory_encoder.pkl\")\n",
    "\n",
    "model = MultiOutputModel(len(gender_enc.classes_), len(color_enc.classes_),\n",
    "                         len(season_enc.classes_), len(product_enc.classes_))\n",
    "model.load_state_dict(torch.load(\"fashion_model.pth\", map_location=device))\n",
    "model = model.to(device).eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÆ Prediction Function\n",
    "def predict(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        results = {\n",
    "            'Gender': gender_enc.inverse_transform([outputs['gender'].argmax(0).item()])[0],\n",
    "            'Color': color_enc.inverse_transform([outputs['color'].argmax(0).item()])[0],\n",
    "            'Season': season_enc.inverse_transform([outputs['season'].argmax(0).item()])[0],\n",
    "            'Product': product_enc.inverse_transform([outputs['product'].argmax(0).item()])[0]\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# üñºÔ∏è Predict from Amazon Screenshot\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/lakshiitakalyanasundaram/Desktop/projects/CodeMonk Assignment/fashion-product-classifier/amazon_screenshots/Screenshot 2025-07-20 at 1.50.43‚ÄØAM.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m      7\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m: gender_enc\u001b[38;5;241m.\u001b[39minverse_transform([\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColor\u001b[39m\u001b[38;5;124m'\u001b[39m: color_enc\u001b[38;5;241m.\u001b[39minverse_transform([outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeason\u001b[39m\u001b[38;5;124m'\u001b[39m: season_enc\u001b[38;5;241m.\u001b[39minverse_transform([outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()])[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct\u001b[39m\u001b[38;5;124m'\u001b[39m: product_enc\u001b[38;5;241m.\u001b[39minverse_transform([outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m     }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# üñºÔ∏è Predict from Amazon Screenshot\n",
    "predict(\"/Users/lakshiitakalyanasundaram/Desktop/projects/CodeMonk Assignment/fashion-product-classifier/amazon_screenshots/Screenshot 2025-07-20 at 1.50.43‚ÄØAM.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
